{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kt_SlKlPbTw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "533856f2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea8b5837",
        "outputId": "d700831f-ed36-40dc-8110-dd1dc0673771"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the wdbc.names file to get column information\n",
        "with open('/content/wdbc.names', 'r') as f:\n",
        "    names_content = f.readlines()\n",
        "\n",
        "#  define the column names based on the typical structure of the WDBC dataset\n",
        "column_names = [\n",
        "    'ID',\n",
        "    'Diagnosis',\n",
        "    'radius_mean',\n",
        "    'texture_mean',\n",
        "    'perimeter_mean',\n",
        "    'area_mean',\n",
        "    'smoothness_mean',\n",
        "    'compactness_mean',\n",
        "    'concavity_mean',\n",
        "    'concave points_mean',\n",
        "    'symmetry_mean',\n",
        "    'fractal_dimension_mean',\n",
        "    'radius_se',\n",
        "    'texture_se',\n",
        "    'perimeter_se',\n",
        "    'area_se',\n",
        "    'smoothness_se',\n",
        "    'compactness_se',\n",
        "    'concavity_se',\n",
        "    'concave points_se',\n",
        "    'symmetry_se',\n",
        "    'fractal_dimension_se',\n",
        "    'radius_worst',\n",
        "    'texture_worst',\n",
        "    'perimeter_worst',\n",
        "    'area_worst',\n",
        "    'smoothness_worst',\n",
        "    'compactness_worst',\n",
        "    'concavity_worst',\n",
        "    'concave points_worst',\n",
        "    'symmetry_worst',\n",
        "    'fractal_dimension_worst'\n",
        "]\n",
        "\n",
        "# Load the wdbc.data dataset using the defined column names\n",
        "df = pd.read_csv('/content/wdbc.data', header=None, names=column_names)\n",
        "\n",
        "print(\"Dataset loaded successfully with custom column names.\")\n",
        "print(df.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully with custom column names.\n",
            "         ID Diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
            "0    842302         M        17.99         10.38          122.80     1001.0   \n",
            "1    842517         M        20.57         17.77          132.90     1326.0   \n",
            "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
            "3  84348301         M        11.42         20.38           77.58      386.1   \n",
            "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
            "\n",
            "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
            "0          0.11840           0.27760          0.3001              0.14710   \n",
            "1          0.08474           0.07864          0.0869              0.07017   \n",
            "2          0.10960           0.15990          0.1974              0.12790   \n",
            "3          0.14250           0.28390          0.2414              0.10520   \n",
            "4          0.10030           0.13280          0.1980              0.10430   \n",
            "\n",
            "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
            "0  ...         25.38          17.33           184.60      2019.0   \n",
            "1  ...         24.99          23.41           158.80      1956.0   \n",
            "2  ...         23.57          25.53           152.50      1709.0   \n",
            "3  ...         14.91          26.50            98.87       567.7   \n",
            "4  ...         22.54          16.67           152.20      1575.0   \n",
            "\n",
            "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   symmetry_worst  fractal_dimension_worst  \n",
            "0          0.4601                  0.11890  \n",
            "1          0.2750                  0.08902  \n",
            "2          0.3613                  0.08758  \n",
            "3          0.6638                  0.17300  \n",
            "4          0.2364                  0.07678  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8243e9e2",
        "outputId": "ed0909bd-f2c0-4ef5-c95c-671d9b34bb3e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Convert 'Diagnosis' column to numerical format (M=1, B=0)\n",
        "df['Diagnosis'] = df['Diagnosis'].map({'M': 1, 'B': 0})\n",
        "\n",
        "# Define target variable (y) and features (X)\n",
        "y = df['Diagnosis']\n",
        "X = df.drop(['ID', 'Diagnosis'], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply StandardScaler to the feature columns\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
        "\n",
        "print(\"Target variable encoded, data split, and features scaled successfully.\")\n",
        "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "print(\"\\nFirst 5 rows of scaled training features:\")\n",
        "print(X_train_scaled.head())\n",
        "print(\"\\nFirst 5 rows of training target variable:\")\n",
        "print(y_train.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target variable encoded, data split, and features scaled successfully.\n",
            "Shape of X_train_scaled: (455, 30)\n",
            "Shape of X_test_scaled: (114, 30)\n",
            "Shape of y_train: (455,)\n",
            "Shape of y_test: (114,)\n",
            "\n",
            "First 5 rows of scaled training features:\n",
            "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
            "68     -1.440753     -0.435319       -1.362085  -1.139118         0.780573   \n",
            "181     1.974096      1.733026        2.091672   1.851973         1.319843   \n",
            "63     -1.399982     -1.249622       -1.345209  -1.109785        -1.332645   \n",
            "248    -0.981797      1.416222       -0.982587  -0.866944         0.059390   \n",
            "60     -1.117700     -1.010259       -1.125002  -0.965942         1.269511   \n",
            "\n",
            "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
            "68           0.718921        2.823135            -0.119150       1.092662   \n",
            "181          3.426275        2.013112             2.665032       2.127004   \n",
            "63          -0.307355       -0.365558            -0.696502       1.930333   \n",
            "248         -0.596788       -0.820203            -0.845115       0.313264   \n",
            "60          -0.439002       -0.983341            -0.930600       3.394436   \n",
            "\n",
            "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
            "68                 2.458173  ...     -1.232861      -0.476309   \n",
            "181                1.558396  ...      2.173314       1.311279   \n",
            "63                 0.954379  ...     -1.295284      -1.040811   \n",
            "248                0.074041  ...     -0.829197       1.593530   \n",
            "60                 0.950213  ...     -1.085129      -1.334616   \n",
            "\n",
            "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
            "68         -1.247920   -0.973968          0.722894           1.186732   \n",
            "181         2.081617    2.137405          0.761928           3.265601   \n",
            "63         -1.245220   -0.999715         -1.438693          -0.548564   \n",
            "248        -0.873572   -0.742947          0.796624          -0.729392   \n",
            "60         -1.117138   -0.896549         -0.174876          -0.995079   \n",
            "\n",
            "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
            "68          4.672828              0.932012        2.097242   \n",
            "181         1.928621              2.698947        1.891161   \n",
            "63         -0.644911             -0.970239        0.597602   \n",
            "248        -0.774950             -0.809483        0.798928   \n",
            "60         -1.209146             -1.354582        1.033544   \n",
            "\n",
            "     fractal_dimension_worst  \n",
            "68                  1.886450  \n",
            "181                 2.497838  \n",
            "63                  0.057894  \n",
            "248                -0.134497  \n",
            "60                 -0.205732  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "\n",
            "First 5 rows of training target variable:\n",
            "68     0\n",
            "181    1\n",
            "63     0\n",
            "248    0\n",
            "60     0\n",
            "Name: Diagnosis, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178621a2",
        "outputId": "a4572298-bd72-4cd2-d62f-0f52bd52ff3a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "log_reg_model = LogisticRegression(random_state=42, solver='liblinear')\n",
        "\n",
        "# Fit the model to the scaled training data\n",
        "log_reg_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained successfully.\")\n",
        "\n",
        "y_train_pred = log_reg_model.predict(X_train_scaled)\n",
        "\n",
        "y_test_pred = log_reg_model.predict(X_test_scaled)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_error = 1 - train_accuracy\n",
        "print(f\"\\nTraining Error: {train_error:.4f}\")\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_error = 1 - test_accuracy\n",
        "print(f\"Test Error: {test_error:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"\\nConfusion Matrix (Test Set):\")\n",
        "print(conf_matrix)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model trained successfully.\n",
            "\n",
            "Training Error: 0.0132\n",
            "Test Error: 0.0263\n",
            "\n",
            "Test Set Performance:\n",
            "Accuracy: 0.9737\n",
            "Precision: 0.9762\n",
            "Recall: 0.9535\n",
            "F1-Score: 0.9647\n",
            "\n",
            "Confusion Matrix (Test Set):\n",
            "[[70  1]\n",
            " [ 2 41]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bff5d07",
        "outputId": "2d95b214-851f-46cb-944a-b88ff3494fea"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "dt_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Decision Tree Classifier model trained successfully.\")\n",
        "\n",
        "y_train_pred_dt = dt_model.predict(X_train_scaled)\n",
        "\n",
        "y_test_pred_dt = dt_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate training accuracy and error\n",
        "train_accuracy_dt = accuracy_score(y_train, y_train_pred_dt)\n",
        "train_error_dt = 1 - train_accuracy_dt\n",
        "print(f\"\\nDecision Tree Training Error: {train_error_dt:.4f}\")\n",
        "\n",
        "# Calculate test accuracy and error\n",
        "test_accuracy_dt = accuracy_score(y_test, y_test_pred_dt)\n",
        "test_error_dt = 1 - test_accuracy_dt\n",
        "print(f\"Decision Tree Test Error: {test_error_dt:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Set Performance (Decision Tree):\")\n",
        "print(f\"Accuracy: {test_accuracy_dt:.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred_dt):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_test_pred_dt):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_test_pred_dt):.4f}\")\n",
        "\n",
        "conf_matrix_dt = confusion_matrix(y_test, y_test_pred_dt)\n",
        "print(\"\\nConfusion Matrix (Test Set - Decision Tree):\")\n",
        "print(conf_matrix_dt)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier model trained successfully.\n",
            "\n",
            "Decision Tree Training Error: 0.0000\n",
            "Decision Tree Test Error: 0.0526\n",
            "\n",
            "Test Set Performance (Decision Tree):\n",
            "Accuracy: 0.9474\n",
            "Precision: 0.9302\n",
            "Recall: 0.9302\n",
            "F1-Score: 0.9302\n",
            "\n",
            "Confusion Matrix (Test Set - Decision Tree):\n",
            "[[68  3]\n",
            " [ 3 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2714ceab"
      },
      "source": [
        "\n",
        "#### Overfitting and Underfitting Analysis:\n",
        "\n",
        "**Logistic Regression**: The training error (1.32%) is very close to the test error (2.63%). This indicates a good balance between bias and variance. The model generalizes well to unseen data, showing no significant signs of overfitting or underfitting. Its performance on the test set is robust across accuracy, precision, recall, and F1-score, suggesting it captures the underlying patterns without memorizing the training data.\n",
        "\n",
        "**Decision Tree Classifier**: The training error is 0.00% (perfect accuracy on the training set), while the test error is 5.26%. This significant gap between training and test performance is a clear indication of **overfitting**. The model has learned the training data too well, including its noise, which has led to a reduction in its ability to generalize to new, unseen data. While still performing reasonably well on the test set, its performance is notably lower than the Logistic Regression model, especially concerning false positives and false negatives.\n",
        "\n",
        "#### Conclusion for Logistic Regression Model:\n",
        "\n",
        "The Logistic Regression model demonstrated strong and consistent performance with low training (1.32%) and test (2.63%) errors, indicating excellent generalization and no significant overfitting. Its high accuracy (97.37%) and robust precision/recall make it a reliable choice for this medical diagnosis task, especially given the criticality of false negatives.\n",
        "\n",
        "#### Conclusion for Decision Tree Classifier:\n",
        "\n",
        "With a perfect 0% training error but a higher 5.26% test error, the Decision Tree Classifier showed clear signs of overfitting, memorizing training data rather than generalizing. Its performance on unseen data was weaker than Logistic Regression, highlighting the need for hyperparameter tuning (e.g., pruning or limiting depth) to improve generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4818e5e7"
      },
      "source": [
        "\n",
        "*   **Relevant Machine Learning Issues**:\n",
        "    *   **Feature Scaling**: Was successfully applied, which is crucial for algorithms like Logistic Regression.\n",
        "    *   **Class Imbalance**: A mild imbalance was noted (approximately 63% Benign, 37% Malignant in training), highlighting the importance of metrics beyond accuracy.\n",
        "    *   **Feature Correlation**: High correlations among features in medical datasets were identified as a potential issue, though not explicitly handled beyond scaling.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZwXzrL0RvIz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}